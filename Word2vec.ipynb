{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqDzOe/te+Jkcl2nHlMy88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saijas2000/Data-science-full/blob/main/Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WzVqWKs1CKG9"
      },
      "outputs": [],
      "source": [
        "# Importing all the libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "r5IuOma2Cd9r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import word2vec,KeyedVectors"
      ],
      "metadata": {
        "id": "VY1avpRcCh3x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(List1,List2):\n",
        "  from numpy import dot\n",
        "  from numpy.linalg import norm\n",
        "  List1 = np.array(List1)\n",
        "  List2 = np.array(List2)\n",
        "  result = dot(List1, List2)/(norm(List1)*norm(List2))\n",
        "  return result"
      ],
      "metadata": {
        "id": "PBGf-OlTC0zW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity([1,2],[2,-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl_xOk9eEQ8L",
        "outputId": "f534ab9f-ee4f-43d0-cad2-e00db956a0a8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVUzuyVWYrr3",
        "outputId": "8513d608-5c43-45b6-8cb4-112513b4690f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=c31d242a70fbe00178ffd82ce66c59478e6991b387288b75364a78117e7889ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "--2022-02-10 11:36:21--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.131.160\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.131.160|:443... connected.\n",
            "HTTP request sent, awaiting response... 206 Partial Content\n",
            "Length: 1647046227 (1.5G), 1396216602 (1.3G) remaining [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[+++================>]   1.53G  62.2MB/s    in 25s     \n",
            "\n",
            "2022-02-10 11:36:46 (53.4 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gzip -d GoogleNews-vectors-negative300.bin.gz"
      ],
      "metadata": {
        "id": "dms2e8mREUe2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import models\n",
        "\n",
        "word_vectors = models.KeyedVectors.load_word2vec_format(\n",
        "    '/content/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "VKWSQUo-FQAS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_model(File):\n",
        "    print(\"Loading Glove Model\")\n",
        "    glove_model = {}\n",
        "    with open(File,'r') as f:\n",
        "        for line in f:\n",
        "            split_line = line.split()\n",
        "            word = split_line[0]\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            glove_model[word] = embedding\n",
        "    print(f\"{len(glove_model)} words loaded!\")\n",
        "    return glove_model\n",
        "  "
      ],
      "metadata": {
        "id": "Tw9igwuADQRx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word_vectors = load_glove_model(\"/content/glove.6B.50d.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHZpt2PdEExB",
        "outputId": "878612d2-aeb2-4244-84ef-54569da93a63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Glove Model\n",
            "400000 words loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors[\"table\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im0xhyUyNOSE",
        "outputId": "bd49acd1-343b-4b7a-8e94-c1d69c95d917"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.14453125 -0.02648926  0.11767578  0.21386719 -0.04223633 -0.02514648\n",
            "  0.12207031 -0.20410156  0.03662109  0.265625   -0.08154297 -0.16699219\n",
            "  0.2109375  -0.08105469 -0.10498047  0.20898438  0.10449219 -0.05004883\n",
            "  0.20800781 -0.05566406  0.20996094  0.13964844 -0.14648438 -0.06982422\n",
            "  0.06103516  0.1484375  -0.1875      0.04321289 -0.11474609 -0.06445312\n",
            " -0.22460938  0.15527344  0.00662231  0.14257812 -0.04370117 -0.11132812\n",
            " -0.11767578  0.20507812 -0.08789062 -0.20996094 -0.07275391 -0.15136719\n",
            "  0.06640625 -0.07080078  0.19433594 -0.04736328 -0.00352478  0.02539062\n",
            "  0.13574219  0.08740234 -0.24023438 -0.01757812 -0.08740234 -0.38867188\n",
            " -0.13183594 -0.09472656  0.20996094 -0.00518799  0.16699219  0.03588867\n",
            " -0.31054688 -0.125      -0.4140625  -0.00738525  0.10058594 -0.0546875\n",
            " -0.04614258 -0.06494141  0.14453125 -0.13378906  0.25976562 -0.21386719\n",
            "  0.265625    0.18652344 -0.04907227 -0.16699219 -0.07666016  0.04833984\n",
            "  0.09619141  0.00052643  0.12255859 -0.35742188  0.0378418   0.11083984\n",
            "  0.25976562 -0.03759766  0.12451172  0.06787109  0.08496094 -0.06835938\n",
            "  0.19628906 -0.01806641 -0.24023438 -0.21679688 -0.13964844 -0.1875\n",
            "  0.12304688 -0.24121094 -0.1484375  -0.2109375  -0.20605469 -0.17773438\n",
            " -0.09863281 -0.13476562 -0.11230469 -0.00854492  0.08105469 -0.08007812\n",
            "  0.06298828 -0.07226562 -0.15332031 -0.04077148  0.23339844 -0.20800781\n",
            " -0.09082031 -0.04199219  0.25       -0.12695312 -0.06494141  0.01696777\n",
            " -0.06030273 -0.00646973  0.15820312  0.11230469 -0.3515625  -0.19433594\n",
            " -0.10058594 -0.1328125  -0.07861328 -0.03149414  0.31640625 -0.07519531\n",
            " -0.16015625 -0.08251953 -0.2109375   0.25        0.00212097 -0.015625\n",
            " -0.12011719  0.19335938 -0.03564453 -0.12890625  0.203125   -0.1015625\n",
            "  0.05688477  0.35742188 -0.32226562 -0.33203125  0.08105469  0.22851562\n",
            "  0.0612793   0.09082031 -0.02563477  0.10009766  0.01708984 -0.05859375\n",
            " -0.0100708  -0.22167969  0.0559082   0.05517578 -0.20898438 -0.06933594\n",
            "  0.04052734 -0.19238281  0.14453125 -0.13671875 -0.07275391  0.06787109\n",
            " -0.15429688 -0.00765991 -0.265625   -0.15136719  0.10644531 -0.03833008\n",
            "  0.0189209  -0.20507812 -0.07373047  0.03955078  0.07373047  0.11083984\n",
            " -0.25585938  0.17382812  0.16894531  0.05981445  0.08691406  0.04394531\n",
            " -0.17871094  0.22167969 -0.02404785 -0.11425781  0.27734375 -0.02954102\n",
            " -0.00108337 -0.06225586  0.0859375   0.140625   -0.21875    -0.0378418\n",
            " -0.13476562 -0.37890625 -0.33789062  0.17578125 -0.15234375 -0.07714844\n",
            "  0.01495361  0.0703125   0.05712891  0.21386719 -0.18457031  0.03027344\n",
            " -0.15039062  0.04638672 -0.21582031 -0.03613281 -0.04467773  0.04931641\n",
            " -0.0703125   0.29101562  0.078125    0.16308594 -0.25585938  0.23730469\n",
            " -0.15722656  0.08544922 -0.14160156 -0.00933838 -0.0300293  -0.046875\n",
            "  0.28320312  0.06884766  0.22558594 -0.16308594  0.09179688  0.08349609\n",
            " -0.12695312 -0.10888672  0.05615234  0.3671875   0.13476562  0.11181641\n",
            " -0.06079102 -0.05029297  0.05102539  0.01867676  0.00047684  0.13183594\n",
            "  0.22070312 -0.11328125 -0.1640625  -0.04711914 -0.0625      0.27539062\n",
            " -0.01745605 -0.25390625 -0.24511719 -0.29882812 -0.29492188 -0.09326172\n",
            " -0.09082031  0.03198242  0.02478027 -0.12792969 -0.16210938 -0.15625\n",
            " -0.19628906  0.20800781 -0.22070312  0.0612793  -0.2578125  -0.21875\n",
            "  0.13671875  0.27929688 -0.05078125  0.02197266 -0.02453613  0.41210938\n",
            "  0.14941406 -0.23632812 -0.08447266 -0.03344727  0.05126953  0.01806641\n",
            "  0.0267334   0.30664062  0.00338745  0.04272461 -0.13476562  0.24121094\n",
            " -0.00805664  0.00302124 -0.03198242 -0.07177734 -0.12304688 -0.0177002\n",
            "  0.00415039 -0.07666016  0.06982422 -0.09521484 -0.09814453 -0.01226807]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def odd_man_out(words):\n",
        "  all_word_vectors = [word_vectors[w] for w in words]\n",
        "  avg_vector = np.sum(all_word_vectors,axis = 0)/5\n",
        "  odd_one_out = None\n",
        "  min_similarity = 10000\n",
        "  for i in range(0,len(words)):\n",
        "    sim = cosine_similarity(avg_vector,all_word_vectors[i])\n",
        "    print(sim,words[i])\n",
        "    if sim<min_similarity:\n",
        "      min_similarity = sim\n",
        "      odd_one_out = i\n",
        "  return odd_one_out"
      ],
      "metadata": {
        "id": "SOQbMJveFcr6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"/content/Test.csv\")\n",
        "df = df.copy()\n",
        "print(df[\"Word1\"][0])\n",
        "lis = [\"Word1\",\"Word2\",\"Word3\",\"Word4\",\"Word5\"]\n",
        "import csv\n",
        "with open(\"output.csv\",\"w\") as f:\n",
        "  csv_writer = csv.writer(f)\n",
        "  csv_writer.writerow(\"OddOne\")\n",
        "  for i in range(0,len(df)):\n",
        "    words = []\n",
        "    for j in range(0,5):\n",
        "      words.append(df[lis[j]][i].lower())\n",
        "    csv_writer.writerow([df[lis[odd_man_out(words)]][i]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE2bjPneHS46",
        "outputId": "f8739cf6-dbef-47c7-e95a-f16e4f4bd44c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elephant\n",
            "0.83574075 elephant\n",
            "0.735615 lion\n",
            "0.8277229 tiger\n",
            "0.7271363 goat\n",
            "0.72673804 snake\n",
            "0.6753332 man\n",
            "0.8031234 policeman\n",
            "0.766102 fireman\n",
            "0.6106567 teacher\n",
            "0.74801856 postman\n",
            "0.6616078 plane\n",
            "0.67476624 bird\n",
            "0.5885772 rocket\n",
            "0.679072 balloon\n",
            "0.5828415 cat\n",
            "0.83195764 onion\n",
            "0.8357253 celery\n",
            "0.8312569 lettuce\n",
            "0.76934993 pineapple\n",
            "0.817367 potato\n",
            "0.4796501 india\n",
            "0.6865719 football\n",
            "0.7336016 hockey\n",
            "0.77386963 cricket\n",
            "0.5593194 swimming\n",
            "0.5429398 who\n",
            "0.8053224 why\n",
            "0.7867594 what\n",
            "0.6749242 where\n",
            "0.47231123 is\n",
            "0.66316915 on\n",
            "0.71537685 in\n",
            "0.64144915 over\n",
            "0.5958207 their\n",
            "0.5409124 was\n",
            "0.83181185 india\n",
            "0.7773033 australia\n",
            "0.7784803 japan\n",
            "0.7964356 russia\n",
            "0.58940876 china\n",
            "0.64894295 dollar\n",
            "0.741219 rupees\n",
            "0.7227386 euros\n",
            "0.70958304 cents\n",
            "0.5120901 money\n",
            "0.75716203 eat\n",
            "0.6918919 sleep\n",
            "0.72600317 drink\n",
            "0.48250487 think\n",
            "0.5506685 dance\n",
            "0.7665576 car\n",
            "0.8693133 scooter\n",
            "0.87597996 bike\n",
            "0.85950255 bicycle\n",
            "0.38604486 ship\n",
            "0.79855424 poland\n",
            "0.82889116 russia\n",
            "0.8075774 england\n",
            "0.73948747 rome\n",
            "0.74117965 ukraine\n",
            "0.8817605 lake\n",
            "0.65220875 sea\n",
            "0.8381007 river\n",
            "0.5900113 pool\n",
            "0.83046174 pond\n",
            "0.616982 sun\n",
            "0.6935276 moon\n",
            "0.4568296 star\n",
            "0.59898335 mars\n",
            "0.52363944 egypt\n",
            "0.8005575 fox\n",
            "0.7906808 wolf\n",
            "0.6926236 jackal\n",
            "0.57555836 mouse\n",
            "0.75719243 panther\n",
            "0.71170807 veil\n",
            "0.7640376 turban\n",
            "0.69161534 helmet\n",
            "0.70198023 shirt\n",
            "0.7274153 hat\n",
            "0.77934355 physics\n",
            "0.6608197 chemistry\n",
            "0.6487152 geography\n",
            "0.70354384 botany\n",
            "0.55486745 universe\n",
            "0.7899715 assassinate\n",
            "0.73795485 kill\n",
            "0.79417557 kidnap\n",
            "0.5758509 stab\n",
            "0.7074022 murder\n",
            "0.7189444 hostel\n",
            "0.8065759 hotel\n",
            "0.7772789 inn\n",
            "0.49770138 club\n",
            "0.80045813 motel\n",
            "0.59816855 earth\n",
            "0.59301215 mars\n",
            "0.4361236 neptune\n",
            "0.5377234 pluto\n",
            "0.61375856 sun\n"
          ]
        }
      ]
    }
  ]
}